<TITLE>Audience evaluations of news videos made with various levels of automation: A population-based survey experiment</TITLE>
<PUBLISHED_DATE>2024-05-08</PUBLISHED_DATE>
<URL>https://journals.sagepub.com/doi/10.1177/14648849241243189</URL>
<AUTHOR>Michael  Koliska</AUTHOR>

<SUMMARY>
This study investigated how UK online news consumers perceive news videos created with varying levels of automation.
-  Researchers conducted a large-scale survey experiment (n=4200) comparing human-made, partially automated, and highly automated videos across 14 different topics.
-  Findings show that while human-made videos generally received slightly more favorable evaluations, the differences were not substantial.
-  Importantly, variations in evaluation were observed depending on the specific news story.
-  The results suggest that partially automated videos with subsequent human editing are well-received.
-  The study also highlights the need for large sample sizes and diverse respondent demographics in future research on this topic.


</SUMMARY>
<HIGHLIGHTS>
- Broadly, these studies have one or both of two main aims.
- Firstly, to compare the perceptions of news texts actually or purportedly generated by humans or machines, and, secondly, to explore factors, mainly psychological, that may explain any variance in these perceptions.
- These factors have included audiences’ expectations of journalists and automated systems ( Waddell, 2018 ); the credibility of the news brand carrying the texts ( Liu and Wei, 2019 ); the level of transparency given about how the news item was automated ( Graefe et al., 2017 ); and the cultural ( Zheng et al., 2018 ), professional ( Jung et al., 2017 ; Van der Kaa and Krahmer, 2014 ), or other demographic ( Melin et al., 2018 ) characteristics of the respondents.
- A meta-analysis ( Graefe and Bohlken, 2020 ) of 12 of the studies published up to October 2019 found that there were no differences in readers’ perceptions of “credibility” between the human- and machine-written news articles.
- However, human-written news was perceived as being of slightly higher “quality” and much more “readable”.

</HIGHLIGHTS>
<CONTENT>
Abstract The use of automation in news content creation is expanding from the written to the audio-visual medium with news organizations including Reuters turning to video automation services provided by companies such as Wibbitz.

Although researchers have explored audience perceptions of text-based news automation, to date no published study has examined how news consumers perceive automated news videos.

We conducted a between-subjects online survey experiment to compare how a socio-demographically representative sample ( n = 4200) of online news consumers in the UK perceived human-made, partly automated, and highly automated short-form online news videos on 14 different story topics.

Our findings show that human-made videos received on average more favourable responses on some evaluation variables, although the differences were not large.

We also found some significant differences in the relative evaluation of automated and human-made news videos across different individual stories.

For practitioners our results suggest partly automated news videos with post-automation human editing can be well received.

For researchers our results show the need to use reasonably large sets of experimental stimuli, and suggest that ensuring socio-demographic variation within samples of respondents is worthwhile.

Introduction The use of automation in journalism is encroaching more and more on what many would consider to be journalists’ core professional practices, such as the identification of story leads, verification, and decisions about which stories are shown, and with what prominence ( Thurman, 2019 ).

Over the past few years, automation has increasingly been used for the creation of written news texts too, and more recently the production of news videos has also become more automated ( Fanta 2017 ).

News organizations including the BBC, Reuters, and The Economist have turned to video automation services provided by companies such as Wibbitz, Wochit, and Synthesia.

Broadly, such services can be driven by textual natural language or structured data as inputs, or can automatically summarize longer video segments, for example whole sports matches ( Merler, 2019 ).

This rise of automation in journalism has been linked to changes in audience demand for content ( Dörr, 2016 ), and the consumption of online news videos has been growing rapidly.

While about 24% of news consumers across 40 countries watched online news videos on a weekly basis in 2016, this proportion had increased to 67% in 2020 ( Newman et al., 2016 , 2020 ).

Video automation has helped to meet this growing demand, and significant time and resources are being invested in the development of applications that introduce elements of automation into the production of audio-visual content, including news.

For example, Wibbitz—whose clients include Reuters, Condé Nast, USA Today , TMZ, and NBC—has raised over US$40 million, including from the Associated Press ( Crunchbase , n.d.), to develop products that, it says, allow the production of video “at unprecedented scale with the power of automation” ( Wibbitz , n.d.).

An important aspect of understanding the impact of these new algorithmic tools within journalism is how audiences perceive news produced using automation.

Although some progress has been made in exploring audience perceptions of text-based news automation (see Graefe and Bohlken, 2020 ; Wang and Huang, 2024 for overviews), as of February 2024 no known study has examined how news consumers perceive automated audio-visual news.

We discuss in the literature review how the visual language of videos, in contrast to written texts, does not follow precise linguistic or widely agreed upon syntactic rules.

It would therefore be unwise to assume that the findings of studies on the perception of automated news texts also apply to automated news videos.

Furthermore, many of the existing studies on the perception of automated journalism have been limited by small and unrepresentative samples of respondents, questionable measures of the dependent variables, and a quantity and quality of experimental stimuli that have made the isolation of authorship (automated or human) as an independent variable problematic.

These limitations make it difficult to know how news consumers really evaluate automated news in comparison with human-made equivalents, and to what extent those judgements might be generalizable.

This study attempts to help fill this twin research gap in two ways.

Firstly, we design and conduct a between-subjects experiment to compare how online news consumers in the UK perceive human-made, partly automated, and highly automated short-form online news videos.

Secondly, we employ a more rigorous methodology than has been seen in the literature to date.

Our choice of country for this study aligned with the authors’ expertise, but we believe that news video consumption habits elsewhere are sufficiently similar (see, e.g., Newman et al., 2020 ) that our findings will be of wider interest.

Literature review “Automated journalism” is largely concerned with the production of news content and may be seen as a sub-category of computational journalism ( Thurman, 2019 ).

Carlson’s (2015) definition of automated journalism as “algorithmic processes that convert data into narrative news texts with limited to no human intervention beyond the initial programming” has been widely adopted but is no longer strictly accurate.

Firstly, automation, as discussed, is now being used to create audio-visual news as well as textual news.

Secondly, journalists have begun to post-edit automated outputs prior to publication.

Although this post-editing is not currently, or ever likely to be, universal, it is happening with increasing frequency ( Thäsler-Kordonouri and Barling, 2023 ).

Thirdly, automated journalism is now working with inputs—like moving images—other than the numeric “data” that Carlson’s (2015) definition implies.

For example, there now exist forms of automated video journalism that, taking raw video footage as input, use artificial intelligence to create highlight videos suitable for broadcast ( Merler, 2019 ).

Other video automation technologies, such as those offered by Synthesia and Wibbitz, can turn text into news videos.

For these reasons, and building on Carlson (2015) , we define automated journalism as: “Algorithmic processes that convert numerical data, images, or text into written or audio-visual news items with various levels of human intervention beyond the initial programming”.

Research into the perception of automated journalism has, at the time of writing in February 2024 and as far as we are aware, focused solely on written news texts.

Broadly, these studies have one or both of two main aims.

Firstly, to compare the perceptions of news texts actually or purportedly generated by humans or machines, and, secondly, to explore factors, mainly psychological, that may explain any variance in these perceptions.

These factors have included audiences’ expectations of journalists and automated systems ( Waddell, 2018 ); the credibility of the news brand carrying the texts ( Liu and Wei, 2019 ); the level of transparency given about how the news item was automated ( Graefe et al., 2017 ); and the cultural ( Zheng et al., 2018 ), professional ( Jung et al., 2017 ; Van der Kaa and Krahmer, 2014 ), or other demographic ( Melin et al., 2018 ) characteristics of the respondents.

A meta-analysis ( Graefe and Bohlken, 2020 ) of 12 of the studies published up to October 2019 found that there were no differences in readers’ perceptions of “credibility” between the human- and machine-written news articles.

However, human-written news was perceived as being of slightly higher “quality” and much more “readable”.

Together, the studies also found that people rated articles more highly across these three criteria if they were told the article was written by a human, even if it was not.

This analysis might suggest that the present study may find some differences in the perception of news videos made manually and with machine assistance, but that those differences may not be great.

However, this may be a simplistic expectation given that the automated production of textual and video news differs a great deal.

Machine-written news texts rely primarily on natural language generation (NLG) systems that are able to manipulate both the semantics (content) and syntax (arrangement) of a written text, according to the linguistic norms and standards of a specific language.

In contrast to written texts, the visual language of videos does not follow similarly precise linguistic or widely agreed upon syntactic rules.

While videographers and filmmakers follow certain conventions in visual storytelling (such as the use of wide shots that provide a sense of location), these conventions can be relatively freely employed.

The lack of strict visual rules means that the algorithmic production of videos takes a different approach to that used for text.

For one variant of algorithmic video production—text-to-video automation—the narrative structure of words (spoken or used as captions) is the starting point, with the algorithm matching the words with the metadata that classify images within a data bank.

The fundamental differences between news texts and videos also mean that the criteria by which they are judged will differ.

Some of the criteria used to compare human- and machine-written news texts, like “pleasant to read” and “well-written” (for an overview see Stalph et al.

2023 ), simply do not apply, while others that have not been used, such as how well images match the captions or the use of audio, may.

Another reason why the results of this study may differ from the findings of prior studies on the perception of automated text journalism is that the quality of the experimental stimuli used in some of those studies has made the isolation of authorship (automated or human) as an independent variable difficult.

For example, Clerwall’s (2014) study compared the evaluations of an automated factual report on an American football game against the evaluations of a human-written opinion column about the prospects of three quarterbacks.

The differences he found between the evaluations of the automated and human-written stories could, therefore, have been partially, or even fully, a consequence of their genre (match report vs opinion) and not wholly or even partially a consequence of their authorship (automated vs human).

Because our object of study differs from previous perception studies of automated journalism, we considered that developing a set of hypotheses from previous literature, using measures that might not be relevant, and setting any expectations about effect sizes, would be premature, and that we needed to adopt a more open, inductive, exploratory approach for this study.

However, since our interest was fundamentally a comparative one, this still indicated an experimental design, in the sense of randomly allocating participants to watch and evaluate differently authored videos, but not in a traditional hypothetico-deductive framing.

Instead of posing hypotheses loosely derived from a slightly different field of enquiry, we decided not to develop and test hypotheses but rather to ask this general research question: RQ1: What, if any, differences exist in UK online news consumers’ evaluations of short-form online news videos made with various levels of automation, and none? Many of the existing studies on the perception of automated journalism use small samples of stimuli.

For example, the studies by Clerwall (2014) and Jung et al.

(2017) each used one pair of stories (on sport), while Wölker and Powell (2021) , Graefe et al.

(2018) , and Li et al.

(2022) each used two pairs of stories.

At the time of writing in February 2024, we are aware of just four studies that have reported experimental results on variations (if any) by story topic in the relative perceptions of automated and manually produced news articles.

The results are somewhat inconsistent.

Jia and Gwizdka (2020 : 106), Jia (2020 : 2624), and Wölker and Powell (2021) found some significant differences, but Haim and Graefe (2017) did not.

As a result, it is far from clear how much a story’s topic will influence the relative evaluations of human and automated news.

Therefore, our second research questions asks: RQ2: How, if at all, do any differences found in RQ1 vary across the 14 story topics (e.g.

Winter Olympics vs cryptocurrency vs Turkish election) included in our experiment? Prior studies on the perception of automated journalism have often used samples of college students that skew young and female.

For example, the respondents in Clerwall’s (2014) and Jia’s (2020) studies had a mean age in the 20–29 range and were predominantly female.

There are some indications, however, that socio-demographic characteristics can make a difference.

For example, Melin et al.

(2018) found that young women in their small sample ( n = 152) liked automated news articles significantly less than older men did.

There is, however, a lack of robust evidence about the variability in how individuals of different socio-demographic groups evaluate automated news.

Comparing how, if at all, our results from RQ1 differ across socio-demographic groups would not only increase the external validity of our findings but could also contribute, more generally, to the evidence about the extent to which, if at all, differences in evaluations of automated and human-made news vary along socio-demographic lines.

Therefore, our third research question asks: RQ3: How, if at all, do any differences found in RQ1 vary according to the gender, age, work status, region, and social grade of respondents? Methodology Experimental design A 14 (story topic) × 3 (level of automation) between-subjects population-based online survey experiment was conducted, with each participant viewing a single video ( n = 100 for each video).

A between-subjects design was considered preferable to a within-subjects design (where participants would watch and evaluate several videos) to avoid results being potentially affected by learning or priming effects.

Given that the sample was drawn from survey panels that included wider ranges of individuals than just engaged college students, 1 we also wanted to set a very low threshold for respondent fatigue.

Keeping the task very short was therefore important.

Since this was an exploratory study, and given the lessons learned from Graefe and Bohlken (2020 , described above), we had no prior information on what effect sizes should be considered noteworthy, which made it impossible to conduct a meaningful power calculation to specify our sample size.

We arrived at n = 100 per video largely pragmatically, choosing a larger sample size than typically used in relevant literature described above.

The study was approved by the Ethics Committee of the School of Arts and Social Sciences at City, University of London.

Stimulus materials Human-made videos The human-made videos were sourced from PA Media (PA), the UK’s national news agency.

The PA “consumer-ready” videos used in this study were captioned, featured no voice-over or presenter, and had been created without the help of automation (Alex Rothwell, PA’s Head of Video, personal communication).

Videos produced using automation The partly and highly automated videos were created by the present study’s researchers using the Wibbitz platform.

At the time the videos were produced, the Wibbitz production process started with text; we used the captions from the human-made videos as our input.

Operators of the platform could decide whether the text should appear as captions or be used as a script for a voice-over.

We chose the former to match the style of the human-made videos.

The platform automatically tried to find media—both video clips and still images—that matched the captions by searching media databases, including those provided by Getty, Reuters, Pond5, and WENN.

Background music was chosen by the researchers.

Although the Wibbitz platform could produce videos of an acceptable quality “out of the box”, with no human intervention beyond the initial text input, most of the videos, in our experience, needed some further editing before they were consumer-ready.

Because the aim of this research project was to compare UK citizens’ evaluations of short-form news videos (1) made by journalists and (2) made using automation, a key issue was the degree of automation used.

The videos produced on the Wibbitz platform could range from the highly automated (relying only on an initial text) through the partly automated (those that have undergone some further human editing) to the minimally automated (those that have undergone heavy further human editing).

Typical use of Wibbitz involved partial automation, rather than high automation, of video production.

In order to reflect the different degrees of automation with which videos on the Wibbitz platform can be produced, we created both highly and partly automated videos.

The process we used is described in the Supplemental Material .

An important point to note is that our post-editing of the highly automated videos to create the partly automated videos mainly involved manually replacing still images and video clips that did not match the captions.

Across the 14 partly automated videos used in our experiment, an average of 55% of the scenes were edited (SD = 12.5) (see Table A in the Supplemental Material).

Because the independent variable in our survey experiment is the degree of automation used to create short-form news videos, it was important to try to minimize the extent to which other variables could have an effect on evaluations, with these variables including the story topic and stylistic elements such as music, editing transitions, and captions.

One way we did this was to use 14 sets of videos covering topics—royalty, sport, celebrity, business, politics, crime, technology, and culture—familiar to UK audiences.

Each set contained a human-made, a partly automated, and a highly automated video on the same story (see Table A in the Supplemental Material for further descriptive information about the videos).

Furthermore, we made sure that both the human-made and automated videos did not differ in their resolution and contained nothing (e.g.

logos) that would give away their authorship.

In addition, we made sure that the human-made and automated videos did not differ significantly stylistically (see Supplemental Material ).

Not all of the PA videos used music: six of the 14 used background sound instead.

The equivalent partly and highly automated videos used music, as no suitable background sound was available.

The length of the videos averaged 63 seconds (SD = 21.8).

The human-made videos were, on average, slightly shorter (M = 56, SD = 16) than the automated videos (M = 66, SD = 24).

The human-made videos relied more on moving images, with eight of the 14 PA videos featuring all moving images and six a mixture of still and moving images.

By contrast, 57% of the automated videos contained just stills and the other 43% a mixture of stills and moving images (see Table A in the Supplemental Material).

Survey development Mindful of Sundar’s (1999) warning about the validity of the dependent variables used in prior news perception studies, we undertook a major project with an inductive (rather than theory-driven) approach to better understand the criteria with which online news consumers evaluate online news videos and to use these findings to develop this study’s survey instrument.

Nine in-depth group interviews—each 2 hours long—were held with a socio-demographically diverse sample of 22 online news video consumers in the UK (recruited via a market research agency).

Each group was shown an average of 4–5 online news videos, both human-made and made with the help of Wibbitz’s automation 

</CONTENT>
